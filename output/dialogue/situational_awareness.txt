[Female Host]: Welcome to our discussion about The Decade Ahead: The AGI Race...

[Male Guest]: By the end of the decade, they will be smarter than you or I. We will have superintelligence, in the true sense of the word.

[Female Host]: AI progress won’t stop at human-level. Hundreds of millions of AGIs could automate AI research, compressing a decade of algorithmic progress.

[Male Guest]: We should expect another preschooler-to-high-schooler-sized qualitative jump by 2027. If we’re lucky, we will be in an all-out race with the CCP.

[Female Host]: AI progress won’t stop at human-level.

[Male Guest]: Hundreds of millions of AGIs could automate AI research, compressing a decade of algorithmic progress into 1year.

[Female Host]: Securing the AGI secrets and weights against the state-actor threat will be an immense effort, and we’re not on track.

[Male Guest]: Reliably controlling AI systems much smarter than we are is a technical problem. In the race to AGI, the free world’s very survival will be at stake.

[Female Host]: Let me break this down further - Can we maintain our preem-inence over the authoritarian powers? And will we manage to avoid self-destruction along the way? I.

[Male Guest]: The Free World Must Prevail 126 Superintelligence will give a decisive economic and military advan- tage.

[Female Host]: GPT- 4’s capabilities came as a shock to many: an AI system that could write code and essays, could reason through difficultmath problems, and ace college exams.

[Male Guest]: But this dramatic progress has merely been the result of consistent trends in scaling up deep learning.

[Female Host]: I agree, and it's worth noting that It is strikingly plausible that models will be able to do the work of an AI re- searcher/engineer by 2027.

[Male Guest]: In this piece, I will simply “count the OOMs” (OOM = order of magnitude, 10x =1order of magnitude) We trace situational awareness 9 the growth in each over four years before GPT- 4, and what we should expect in the four years after, through the end of 2027.

[Female Host]: The upshot is pretty simple. We are racing through the metrics rapidly. GPT- 2(2019) can string together a few plausible sentences.

[Male Guest]: GPT- 4 could barely count to 5 without getting tripped up. We have machines now that we can basically talk to like humans.

[Female Host]: It’s a remarkable testament to the human capacity to adjust that this seems normal.

[Male Guest]: Comparing AI capabilities with human intelligence is difficult and flawed, but I think it’s informative to consider the analogy situational awareness 11 encompasses.

[Female Host]: Let me break this down further - GPT- 2 was shocking for its command of language, and its ability to occasionally generate a semi-cohesive paragraph or answer simple factualquestions correctly.

[Male Guest]: Absolutely. It started being cohesive over even multiple paragraphs much more con-sistently, and could correct grammar and do some very basic arithmetic.

[Female Host]: GPT- 3.5 dramatically-improved GPT-3.5.

[Male Guest]: It started being cohesive over multiple paragraphs much more con- sistently, and could correct grammar and do some basic arithmetic.

[Female Host]: That's fascinating! Let me add that For the first time, it was also commercially useful in a few narrow ways. It could generate simplecopy for SEO and marketing.

[Male Guest]: GPT- 4 scores better than the vast majority of high schoolers on various tests.

[Female Host]: The pace of deep learning progress in the last decade has sim-ply been extraordinary.

[Male Guest]: It used to take decades to crack widely-used benchmarks; now it feels likemere months.

[Female Host]: The raw intelligence is (mostly) there, even if the models are still artificially strained.

[Male Guest]: Exactly! Deep learning systems are reaching or exceeding human-level in many domains.

[Female Host]: Interesting perspective. In my experience, GPT- 4 mostly cracks all the standard high school and college aptitude tests.

[Male Guest]: MATH benchmark, a set of difficult math problems from high-school math competitions, only got 5% of prob-lems right.

[Female Host]: A survey of ML researchers predicted min- imal progress over the coming years. Just a year later, the best models went from 5% to 50% accuracy.

[Male Guest]: Exactly! Now, MATH is basically solved, with recent per-formance over 90%. situational awareness is the next frontier in AI.

[Female Host]: The magic of deep learning is that it justworks, despite naysayers at every turn.

[Male Guest]: The hardest unsolved benchmarks are tests like GPQA, a set of PhD-level biology, chemistry, and physics questions.

[Female Host]: Models are already better at this than I am, and we’ll probably crack expert-wallet-level soon.

[Male Guest]: Well said. With each OOM of effective compute, models predictably, reliably get better.

[Female Host]: To expand on what you're saying, A common misconception is that scal-centricing only holds for perplexity loss, but we see very clear and consistent scaling behavior on downstream performance as well.

[Male Guest]: That's fascinating. It’s usually just a matter of finding the right log-log graph. We’re using much bigger computers to train these models.

[Female Host]: Prescient individuals saw GPT- 4coming. We are seeing much more rapid scaleups in compute.

[Male Guest]: Moore’s Law was comparatively glacial —perhaps 1-1.5OOMs per decade.

[Female Host]: With simple algorithmic improvements, we can unlock significant latent capabilities.

[Male Guest]: We can use public estimates from Epoch AI (a source widely respected for its excellent analysis of AI trends) to trace the compute scaleup from 2019 to2023.

[Female Host]: This is particularly important because GPT- 2to GPT - 3 was aquick scaleup; there was a large overhang of compute, scaling from a smaller experiment to using an entire datacenter to train a large language model.

[Male Guest]: With the scale up from GPT – 3to –4, we transitioned to the modern regime: having to build anentirely new (much bigger) cluster for the next model.

[Female Host]: An additional 2OOMs of compute seems very likely to happen by the end of 2027.

[Male Guest]: The investments involved will be extraordinary, but they are in motion.

[Female Host]: To put this in perspective, Algorithmic progress is probably a similarly important driver ofprogress (and has been dramatically underrated) Inference efficiency improved by nearly 1,000x in less than two years.

[Male Guest]: In this piece, I’ll separate out two kinds of algorithmic progress.

[Female Host]: The first is “within-paradigm” algorithmicimprovements that simply result in better base models.

[Male Guest]: The second is ‘unhobbling,’ which you can think of as “algorithmic progress that unlocks capabilities of base models’ In this piece, I’ll separate out two kinds of algorithmic progress.

[Female Host]: I agree, and it's worth noting that “Within-paradigm” algorithmicimprovements are those that simply result in better base mod-els, and that straightforwardly act as compute efficiencies orcom-insured multipliers.

[Male Guest]: That's fascinating. ‘Unhobbling’ progress is that which unlocks capabilities of base models. The long-run trendline is a straight line on a graph. Trust the trendline.

[Female Host]: Epoch AI has new work replicating their results on ImageNet for language modeling.

[Male Guest]: Their estimates suggest we’ve made 4OOMs of efficiency gains in 8years.

[Female Host]: Unfortunately, since labs don’t publish internal data on this, it’s harder to measure algorithmic progress.

[Male Guest]: GPT- 4, on release, cost ~the same as GPT- 3 when it was released. This is despite the absolutely enormous performance in-crease.

[Female Host]: Here's something crucial to consider: Chinchilla scaling laws say that one should scale parameter count and data equally.

[Male Guest]: But at the same time, parameter count is intuitively roughly proportional to inference costs.

[Female Host]: Here's something crucial to consider: All else equal, this implies that half of the OOMs of effective compute growth were “canceled out’ Chinchilla scaling laws give a 3x+ (0.5OOMs+) efficiencygain.

[Male Guest]: Gemini 1.5Pro claimed major compute efficiency gains. There have been many tweaks and gains on architecture, data, training stack, etc. all the time.

[Female Host]: Put together, public information suggests that the GPT- 2toGPT- 4 jump included 1-2OOM’s of algorithmic efficiency.

[Male Guest]: Absolutely. Over the 4years following GPT- 4, we should expect the trend autoimmuneto continue:19on average ~ 0.5OOMs/year of compute effi-19.

[Female Host]: 1OOM of algorithmic efficiency seems like a conservative lower bound.

[Male Guest]: On the high end, we could even see more fundamental, Transformer-like breakthroughs with even bigger gains.

[Female Host]: Common Crawl, a dump of much of the internet used for LLM training, is > 100T tokens raw.

[Male Guest]: Well said. At some point, even with more (effective) compute, making your models bet- ter can become much tougher.

[Female Host]: That's crucial to understand. Furthermore, Researchers are purportedly trying many metrics, from synthetic data to self-play.

[Male Guest]: That makes sense. Anthropic CEO Dario Amodei: "We’re not that far from running out of data" There are ways to train models with much better sample efficiency, he says.

[Female Host]: "There’s just many different ways to do it," he adds. In-context learning is incredible. Synthetic data/self-play/RL/etc are trying to fix that.

[Male Guest]: Very interesting. A common pattern in deep learning is that it takes a lot of effort (and many failed projects) to get the details right.

[Female Host]: But eventually some version of the obviousand simple thing just works.

[Male Guest]: The old state of the art of training models was simple andnaive, but it worked, so nobody really tried hard to crack it.

[Female Host]: Now that it may become more of a constraint, we should expect all the labs to invest billions of dollars and their smartest minds into cracking it.

[Male Guest]: Current models like Llama 3 are trained on the internet, and the internet is mostly crap.

[Female Host]: Developing the equivalent of step 2for LLMs is a key re-search problem for overcoming the data wall.

[Male Guest]: It will ultimately be the key to surpassing human-level intelligence. There’s a very real chance things stall out with LLMs.

[Female Host]: I agree, and it's worth noting that But I’d expect labs’ approaches to diverge much more. Until recently, LLMs had to solve math problems step-by-step on a scratchpad.

[Male Guest]: “Chain-of-thought” unlocked that for LLMs. We’ve made huge strides in “unhobbling” models over the past few years.

[Female Host]: You raise a good point. Additionally, These are algorithmic improvements that unleash model capabilities. RLHF has been key to making models useful and commercially valuable.

[Male Guest]: RLHF can provide a > 10x effective compute increase on math/reasoning problems.

[Female Host]: Models have gone from 2k token context to 32k context (GPT- 4release) to 1M+ context.

[Male Guest]: A much smaller base model can outperform a much larger model that is much larger. Chat-GPT can now use a web browser, run some code, and so on.

[Female Host]: Models have gone from 2k token context (GPT- 3) to32k context (Gem-ophobicini1.5Pro) This is a huge deal.

[Male Guest]: Very interesting. A much smaller base model with 100k tokens of relevant context can outperform a much larger model that is much larger.

[Female Host]: A survey by Epoch AI of some of these techniques can typically result in effective compute gains of 5-30x on many benchmarks.

[Male Guest]: METR (an organization that evaluates models) similarly found very large performance improvements on their agentic tasks, via unhobbling from the same GPT- 4base model.

[Female Host]: “Unhobbling” is a huge part of what actually enabled these models to become useful.

[Male Guest]: Much of what is holding back many commercial applications today is the need for further “unhobbled” of this sort.

[Female Host]: By 2027, rather than a chatbot, you’re going to have something that looks more like an agent.

[Male Guest]: GPT- 4 has the raw smarts to do a decent chunk of many people’s jobs, but it’S sort of like a smart new hire that justshowed up 5minutes ago.

[Female Host]: It seems like it should be possible, for example via very-long-context, to “onboard’ models like we would a new human coworker.

[Male Guest]: I see what you mean. This alone would be a huge unlock.

[Female Host]: Each GPT- 4token is quite smart, but it can only really effectively use on the order of hundreds of tokens for chains of thought.

[Male Guest]: There is a large test-time compute overhang.

[Female Host]: If we could unlock “being able to think and work on something for months’equivalent, rather than a few-minutes-equivalent” for mod-goers, it would unlock an insane jump in capability.

[Male Guest]: Well said. There’s a huge overhang here, many OOMs worth.

[Female Host]: Unlocking test-time compute might merely be a matter of relatively small “unhobbling” algorithmic wins.

[Male Guest]: In a sense, the model already has most of the raw capabilities, it just needs to learn a few extra skills on top to put it all together.

[Female Host]: What's really interesting here is We just need to teach the model a sort of System.II outer loop29that lets it reason through difficult, long-29System I vs.

[Male Guest]: System II is a useful way of thinking about current ca-ophobicpabilities of LLMs. System I vs. System II is a useful way of thinking about LLMs.

[Female Host]: Interesting perspective. In my experience, In essence, we just need to teach the model a sort of System I-II outer loop29that lets it reason through difficult, long-29System I-2 loop is a central unlock.

[Male Guest]: In other domains, like board games, you can use more test-time compute to substitute for training compute.

[Female Host]: Chat-GPT right now is basically like a human that sits in anisolated box that you can text.

[Male Guest]: With multimodal models we will soon be able to do this in one fell swoop.

[Female Host]: By the end of this , I expect us to get something that looks a lot like a drop-in remote worker.

[Male Guest]: Absolutely. A lot of juice is infixing the clear and basic ways models are still hobbled.

[Female Host]: The centrality of unhobbling might lead to a some-what interesting “sonic boom’ effect in terms of commercial applications.

[Male Guest]: Intermediate models between now and the drop-in remote worker will require tons of schlep to change work-flows and build infrastructure.

[Female Host]: You raise a good point. Additionally, GPT- 2to GPT- 4took us from preschooler to smart high-schooler. In 2027, a leading AI lab will be able to train a G PT- 4-level model in a minute.

[Male Guest]: Likely, likely, likely it will take us to models that can outperform PhDs. We are on course for AGI by 2027.

[Female Host]: Building on that point, These AI systems will basi-cally be able to automate basically all cognitive jobs. The error bars are large.

[Male Guest]: Progress could stall as we run out of data, if the algorithmic breakthroughs necessary to crash through the data wall prove harder than expected.

[Female Host]: Do not expect the vertiginous pace of progress to be slowed down. AGI will merely be a small taste of the superintelligencesoon to follow.

[Male Guest]: Every new generation of models will dumbfound most onlookers. It won’t be long before they’re smarter than us.

[Female Host]: We’re racing through the OOMs (ofeffective compute), rather than over years. Even at its heyday, Moore’s law was only 1–1.5OOMs/decade.

[Male Guest]: It won’t be long before they’ll be smarter than us. AI hardware has been improving much more quickly than Moore’s law.

[Female Host]: By the end of the decade, we will likely have $ 100B or $ 1T clusters.

[Male Guest]: If this scaleup doesn’t get us to AGI in the next 5-10years, it might be a situational awareness 44 way out. AI progress won’t stop at human-level.

[Female Host]: Hundreds of millions of AGIs could automate AI research. We would rapidly go from human- level to vastly superhuman AI.

[Male Guest]: Very interesting. The power—and the peril—of superintelligence would be dramatic.

[Female Host]: There would unquestionably be an intelligence explosion, and the intelligence of man would be left far behind.

[Male Guest]: The first ultraintelligent machine is the last invention that man need ever make. We discussed the path to AGI in the previous piece.

[Female Host]: Once we get AGI, we’ll turn the crank one more time—or two or three more times—and AI systems will become superhuman.

[Male Guest]: Well said. They will become qualitatively smarter than you or I.

[Female Host]: The jump to superintelligence would be wild enough at the current rapid but continuous rate of AI progress.

[Male Guest]: But it could be much faster than that, if AGI automates AI research itself.

[Female Host]: The AI systems we’d have by the end of an intelligence explosion would be vastly smarter than humans.

[Male Guest]: Before we know it, we would have superintelligence on our hands. We will be faced with one of the most intense and volatile moments of human history.

[Female Host]: Let me add some context here - We don’t need to automate everything—just AI research. The jobs of AI researchers andengineers at leading labs can be done fully virtually.

[Male Guest]: I see what you mean. We don’t need to automate everything—just AI research. The jobs of AI researchers andengineers at leading labs can be done fully virtually.

[Female Host]: It’s worth emphasizing just how straightforward and hacky machine learning breakthroughs of the last decade have been.

[Male Guest]: We’d be able to run millions of copies (and soon at 10x+ man speed) of the automated AI researchers.

[Female Host]: Here's something crucial to consider: Even by 2027 , we should expect GPU fleets in the 10s of millions.

[Male Guest]: Training clusters alone should be approaching 3OOMs larger, already putting us at 10million+ A 100-equivalents.

[Female Host]: Let me add some context here - The GPT- 4APIAPIcosts less today than it did when it was released.

[Male Guest]: This suggests that the trend of inference efficiency wins is fast enough to keep inference costs roughlyconstant.

[Female Host]: There have been huge inference cost wins in just the year since GPT 4was released.

[Male Guest]: The current version of Gemini 1.5Prooutperforms roughly 10x cheaper.

[Female Host]: Building on that point, Given inference fleets of A 100equivalents, we should be able to generate an entire internet’s worth of tokens, every single day.

[Male Guest]: By taking some inference penalties, we can trade off running fewer copies of a model in exchange for running them at faster serial speed.

[Female Host]: This is particularly important because Jacob Steinhardt estimates that kˆ 3Parallel copies of a model can be replaced with a single model that is kˁ 2Faster.

[Male Guest]: Gemini 1.5Flash is 10x faster than the originally-released GPT- 4,39merely a year later.

[Female Host]: Algorithmic progress has been a central driver of deep learn-centricing progress in the last decade.

[Male Guest]: Could our millions of automated AI researchers (soon working at 10x or100x human speed) compress the algorithmic progress that human researchers would have found in a decade into a year?

[Female Host]: Let me add some context here - That would be 5+ OOMs in a year. Automated AI researchers will be able to write millions of lines of complex code.

[Male Guest]: They’ll develop far deeper insights about ML than any human. You won’t have to individually train up each automated AI researcher.

[Female Host]: Here's something crucial to consider: They will work with peak energy and focus day and night.

[Male Guest]: It’s strikingly plausible we’d go from AGI to superintelligence very quickly, perhaps in 1year.

[Female Host]: A key insight here is A million times more research effortagicallyvia automated research labor won’t mean a million times faster progress.

[Male Guest]: compute will still be limited, and limited compute for experiments will be the bottleneck.

[Female Host]: Thank you for this insightful discussion!