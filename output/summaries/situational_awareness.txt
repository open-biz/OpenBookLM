The Decade Ahead: The AGI Race has begun.
By 2025 /26, these machines will outpace college grad-uates.
By the end of the decade, they will be smarter than you or I.
We will have superintelligence, in the true sense of the word.
AI progress won’t stop at human-level.
Hundreds of millions of AGIs could automate AI research, compressing a decade of algorithmic progress.
We should expect another preschooler-to-high-schooler-sized qualitative jump by 2027.
If we’re lucky, we will be in an all-out race with the CCP.
AI progress won’t stop at human-level.
Hundreds of millions of AGIs could automate AI research, compressing a decade of algorithmic progress into 1year.
Securing the AGI secrets and weights against the state-actor threat will be an immense effort, and we’re not on track.
Reliably controlling AI systems much smarter than we are is a technical problem.
In the race to AGI, the free world’s very survival will be at stake.
Can we maintain our preem-inence over the authoritarian powers?
And will we manage to avoid self-destruction along the way?
I.
The Free World Must Prevail 126                 Superintelligence will give a decisive economic and military advan-                tage.
GPT- 4’s capabilities came as a shock to many: an AI system                that could write code and essays, could reason through difficultmath problems, and ace college exams.
But this dramatic progress has merely been the result of consistent trends in scaling up deep learning.
It is strikingly plausible that models will be able to do the work of an AI re-                searcher/engineer by 2027.
In this piece, I will simply “count the OOMs” (OOM = order of magnitude, 10x =1order of magnitude) We trace situational awareness 9                the growth in each over four years before GPT- 4, and what we should expect in the four years after, through the end of 2027.
The upshot is pretty simple.
We are racing through the metrics rapidly.
GPT- 2(2019) can string together a few plausible sentences.
GPT- 4 could barely count to 5 without getting tripped up.
We have machines now that we can basically talk to like humans.
It’s a remarkable testament to the human capacity to adjust that this seems normal.
Comparing AI capabilities with human intelligence is difficult and flawed, but I think it’s informative to consider the analogy situational awareness 11 encompasses.
GPT- 2 was shocking for its command of language, and its ability to occasionally generate a semi-cohesive paragraph or answer simple factualquestions correctly.
It started being cohesive over even multiple paragraphs much more con-sistently, and could correct grammar and do some very basic arithmetic.
GPT- 3.5 dramatically-improved GPT-3.5.
It started being cohesive over multiple paragraphs much more con-                sistently, and could correct grammar and do some basic arithmetic.
For the first time, it was also commercially useful in a few narrow ways.
It could generate simplecopy for SEO and marketing.
GPT- 4 scores better than the vast majority of high schoolers on various tests.
The pace of deep learning progress in the last decade has sim-ply been extraordinary.
It used to take decades to crack widely-used benchmarks; now it feels likemere months.
The raw intelligence is (mostly) there, even if the models are still artificially strained.
Deep learning systems are reaching or exceeding human-level in many domains.
GPT- 4 mostly cracks all the standard high school and college aptitude tests.
MATH benchmark, a set of difficult math problems from high-school math competitions, only got 5% of prob-lems right.
A survey of ML researchers predicted min-                imal progress over the coming years.
Just a year later, the best models went from 5% to 50% accuracy.
Now, MATH is basically solved, with recent per-formance over 90%.
situational awareness is the next frontier in AI.
The magic of deep learning is that it justworks, despite naysayers at every turn.
The hardest unsolved benchmarks are tests like GPQA, a set of PhD-level biology, chemistry, and physics questions.
Models are already better at this than I am, and we’ll probably crack expert-wallet-level soon.
With each OOM of effective compute, models predictably, reliably get better.
A common misconception is that scal-centricing only holds for perplexity loss, but we see very clear and consistent scaling behavior on downstream performance as well.
It’s usually just a matter of finding the right log-log graph.
We’re using much bigger computers to train these models.
Prescient individuals saw GPT- 4coming.
We are seeing much more rapid scaleups in compute.
Moore’s Law was comparatively glacial —perhaps 1-1.5OOMs per decade.
With simple algorithmic improvements, we can unlock significant latent capabilities.
We can use public estimates from Epoch AI (a source widely respected for its excellent analysis of AI trends) to trace the compute scaleup from 2019 to2023.
GPT- 2to GPT - 3 was aquick scaleup; there was a large overhang of compute, scaling from a smaller experiment to using an entire datacenter to train a large language model.
With the scale up from GPT – 3to –4, we transitioned to the modern regime: having to build anentirely new (much bigger) cluster for the next model.
An additional 2OOMs of compute seems very likely to happen by the end of 2027.
The investments involved will be extraordinary, but they are in motion.
Algorithmic progress is probably a similarly important driver ofprogress (and has been dramatically underrated) Inference efficiency improved by nearly 1,000x in less than two years.
In this piece, I’ll separate out two kinds of algorithmic progress.
The first is “within-paradigm” algorithmicimprovements that simply result in better base models.
The second is ‘unhobbling,’ which you can think of as “algorithmic progress that unlocks capabilities of base models’
In this piece, I’ll separate out two kinds of algorithmic progress.
“Within-paradigm” algorithmicimprovements are those that simply result in better base mod-els, and that straightforwardly act as compute efficiencies orcom-insured multipliers.
‘Unhobbling’ progress is that which unlocks capabilities of base models.
The long-run trendline is a straight line on a graph.
Trust the trendline.
Epoch AI has new work replicating their results on ImageNet for language modeling.
Their estimates suggest we’ve made 4OOMs of efficiency gains in 8years.
Unfortunately, since labs don’t publish internal data on this, it’s harder to measure algorithmic progress.
GPT- 4, on release, cost ~the same as GPT- 3 when it was released.
This is despite the absolutely enormous performance in-crease.
Chinchilla scaling laws say that one should scale parameter count and data equally.
But at the same time, parameter count is intuitively roughly proportional to inference costs.
All else equal, this implies that half of the OOMs of effective compute growth were “canceled out’ Chinchilla scaling laws give a 3x+ (0.5OOMs+) efficiencygain.
Gemini 1.5Pro claimed major compute efficiency gains.
There have been many tweaks and gains on architecture, data, training stack, etc.
all the time.
Put together, public information suggests that the GPT- 2toGPT- 4 jump included 1-2OOM’s of algorithmic efficiency.
Over the 4years following GPT- 4, we should expect the trend autoimmuneto continue:19on average ~ 0.5OOMs/year of compute effi-19.
1OOM of algorithmic efficiency seems like a conservative lower bound.
On the high end, we could even see more fundamental, Transformer-like breakthroughs with even bigger gains.
Common Crawl, a dump of much of the internet used for LLM training, is > 100T tokens raw.
At some point, even with more (effective) compute, making your models bet-                ter can become much tougher.
Researchers are purportedly trying many metrics, from synthetic data to self-play.
Anthropic CEO Dario Amodei: "We’re not that far from running out of data" There are ways to train models with much better sample efficiency, he says.
"There’s just many different ways to do it," he adds.
In-context learning is incredible.
Synthetic data/self-play/RL/etc are trying to fix that.
A common pattern in deep learning is that it takes a lot of effort (and many failed projects) to get the details right.
But eventually some version of the obviousand simple thing just works.
The old state of the art of training models was simple andnaive, but it worked, so nobody really tried hard to crack it.
Now that it may become more of a constraint, we should expect all the labs to invest billions of dollars and their smartest minds into cracking it.
Current models like Llama 3 are trained on the internet, and the internet is mostly crap.
Developing the equivalent of step 2for LLMs is a key re-search problem for overcoming the data wall.
It will ultimately be the key to surpassing human-level intelligence.
There’s a very real chance things stall out with LLMs.
But I’d expect labs’ approaches to diverge much more.
Until recently, LLMs had to solve math problems step-by-step on a scratchpad.
“Chain-of-thought” unlocked that for LLMs.
We’ve made huge strides in “unhobbling” models over the past few years.
These are algorithmic improvements that unleash model capabilities.
RLHF has been key to making models useful and commercially valuable.
RLHF can provide a > 10x effective compute increase on math/reasoning problems.
Models have gone from 2k token context to 32k context (GPT- 4release) to 1M+ context.
A much smaller base model can outperform a much larger model that is much larger.
Chat-GPT can now use a web browser, run some code, and so on.
Models have gone from 2k token context                (GPT- 3) to32k context (Gem-ophobicini1.5Pro) This is a huge deal.
A much smaller base model with 100k tokens of relevant context can outperform a much larger model that is much larger.
A survey by Epoch AI of some of these techniques can typically result in effective compute gains of 5-30x on many benchmarks.
METR (an organization that evaluates models) similarly found very large performance improvements on their agentic tasks, via unhobbling from the same GPT- 4base model.
“Unhobbling” is a huge part of what actually enabled these models to become useful.
Much of what is holding back many commercial applications today is the need for further “unhobbled” of this sort.
By 2027, rather than a chatbot, you’re going to have something that looks more like an agent.
GPT- 4 has the raw smarts to do a decent chunk of many people’s jobs, but it’S sort of like a smart new hire that justshowed up 5minutes ago.
It seems like it should be possible, for example via very-long-context, to “onboard’                models like we would a new human coworker.
This alone would be a huge unlock.
Each GPT- 4token is quite smart, but it can only really effectively use on the order of hundreds of tokens for chains of thought.
There is a large test-time compute overhang.
If we could unlock “being able to think and work on something for months’equivalent, rather than a few-minutes-equivalent” for mod-goers, it would unlock an insane jump in capability.
There’s a huge overhang here, many OOMs worth.
Unlocking test-time compute might merely be a matter of relatively small “unhobbling” algorithmic wins.
In a sense, the model already has most of the raw capabilities, it just needs to learn a few extra skills on top to put it all together.
We just need to teach the model a sort of System.II outer loop29that lets it reason through difficult, long-29System I vs.
System II is a useful way of thinking about current ca-ophobicpabilities of LLMs.
System I vs.
System II is a useful way of thinking about LLMs.
In essence, we just need to teach the model a sort of System I-II outer loop29that lets it reason through difficult, long-29System I-2 loop is a central unlock.
In other domains, like board games, you can use more test-time compute to substitute for training compute.
Chat-GPT right now is basically like a human that sits in anisolated box that you can text.
With multimodal models we will soon be able to do this in one fell swoop.
By the end of this , I expect us to get something that looks a lot like a drop-in remote worker.
A lot of juice is infixing the clear and basic ways models are still hobbled.
The centrality of unhobbling might lead to a some-what interesting “sonic boom’ effect in terms of commercial applications.
Intermediate models between now and the drop-in remote worker will require tons of schlep to change work-flows and build infrastructure.
GPT- 2to GPT- 4took us from preschooler to smart high-schooler.
In 2027, a leading AI lab will be able to train a G PT- 4-level model in a minute.
Likely, likely, likely it will take us to models that can outperform PhDs.
We are on course for AGI by 2027.
These AI systems will basi-cally be able to automate basically all cognitive jobs.
The error bars are large.
Progress could stall as we run out of data, if the algorithmic breakthroughs necessary to crash through the data wall prove harder than expected.
Do not expect the vertiginous pace of progress to be slowed down.
AGI will merely be a small taste of the superintelligencesoon to follow.
Every new generation of models will dumbfound most onlookers.
It won’t be long before they’re smarter than us.
We’re racing through the OOMs (ofeffective compute), rather than over years.
Even at its heyday, Moore’s law was only 1–1.5OOMs/decade.
It won’t be long before they’ll be smarter than us.
AI hardware has been improving much more quickly than Moore’s law.
By the end of the decade, we will likely have $ 100B or $ 1T clusters.
If this scaleup doesn’t get us to AGI in the next 5-10years, it might be a situational awareness 44 way out.
AI progress won’t stop at human-level.
Hundreds of millions of AGIs could automate AI research.
We would rapidly go from human- level to vastly superhuman AI.
The power—and the peril—of superintelligence would be dramatic.
There would unquestionably be an intelligence explosion, and the intelligence of man would be left far behind.
The first ultraintelligent machine is the last invention that man need ever make.
We discussed the path to AGI in the previous piece.
Once we get AGI, we’ll turn the crank one more time—or two or three more times—and AI systems will become superhuman.
They will become qualitatively smarter than you or I.
The jump to superintelligence would be wild enough at the current rapid but continuous rate of AI progress.
But it could be much faster than that, if AGI automates AI research itself.
The AI systems we’d have by the end of an intelligence explosion would be vastly smarter than humans.
Before we know it, we would have superintelligence on our hands.
We will be faced with one of the most intense and volatile moments of human history.
We don’t need to automate everything—just AI research.
The jobs of AI researchers andengineers at leading labs can be done fully virtually.
We don’t need to automate everything—just AI research.
The jobs of AI researchers andengineers at leading labs can be done fully virtually.
It’s worth emphasizing just how straightforward and hacky machine learning breakthroughs of the last decade have been.
We’d be able to run millions of copies (and soon at 10x+ man speed) of the automated AI researchers.
Even by 2027 , we should expect GPU fleets in the 10s of millions.
Training clusters alone should be approaching 3OOMs larger, already putting us at 10million+ A 100-equivalents.
The GPT- 4APIAPIcosts less today than it did when it was released.
This suggests that the trend of inference efficiency wins is fast enough to keep inference costs roughlyconstant.
There have been huge inference cost wins in just the year since GPT 4was released.
The current version of Gemini 1.5Prooutperforms roughly 10x cheaper.
Given inference fleets of A 100equivalents, we should be able to generate an entire internet’s worth of                tokens, every single day.
By taking some inference penalties, we can trade off running fewer copies of a model in exchange for running them at faster serial speed.
Jacob Steinhardt estimates that kˆ 3Parallel copies of a model can be replaced with a single model that is kˁ 2Faster.
Gemini 1.5Flash is 10x faster than the originally-released GPT- 4,39merely a year later.
Algorithmic progress has been a central driver of deep learn-centricing progress in the last decade.
Could our millions of automated AI researchers (soon working at 10x or100x human speed) compress the algorithmic progress that human researchers would have found in a decade into a year?
That would be 5+ OOMs in a year.
Automated AI researchers will be able to write millions of lines of complex code.
They’ll develop far deeper insights about ML than any human.
You won’t have to individually train up each automated AI researcher.
They will work with peak energy and focus day and night.
It’s strikingly plausible we’d go from AGI to superintelligence very quickly, perhaps in 1year.
A million times more research effortagicallyvia automated research labor won’t mean a million times faster progress.
compute will still be limited, and limited compute for experiments will be the bottleneck.
Some human researchers and engineers are able to produce 10x the progress as others, even with the same amount of compute.
Human AI researchers would remain a major bottleneck, making the overall increase in the rate of algo-rithmic progress relatively small.
There’s likely some long tail of capabilities required for automating AI research.
Maybe another 5OOMs of algorithmic efficiency will be fundamentally im-                possible?
I doubt it.
Biologicalreference classes also support dramatically more efficient algorithms being plausible.Ideas get harder to find, so the automated AI researchers will merely sustain, rather than accelerate, the current rate of progress.
As we exhaust the low-hanging fruit, it will take moreand more effort to sustain that progress.
In econ modeling terms, it’s a “knife-edge assumption’ to assume that the increase in research effort from automation will be just enough to keep progress constant.
The sheer size of the one-time boost probably overcomes diminishing returns.
A year, or at most just a few years, in which we go from fully-automated AI researchers                to vastly superhuman AI systems should be our mainline ex-pectation.
The millions of automated AI researchers won’t have any more compute to run their experiments on than human AI researchers; perhaps they’ll just be sitting around waiting for their jobs to finish.
With 5OOMs of baseline scaleup in the next four years, “small scale” will mean GPT- 4scale.
Automated AI researchers will be able to run 100,000GPT-4-level experiments on their training cluster in a year.
(That’s a lot situational awareness 57) The automated AI researchers could be way more efficient.
They could make smaller models with similar performance in relevant domains.
Every OOM of training gains they find will give them an OOM situational awareness 58% more of effective compute to run experiments on.
It’s hard to understate how many fewer experiments you would have to run if you got it right on the first try.
The automated AI researchers could have way better intuitions.
on the first try, and only run high value-of-information                experiments.
The people who can do this are surely 10-100x AI researchers, says Jason Wei.Compute bottlenecks will mean a million times more cognitive power won’t translate into an overnight intelligence explosion.
James Bradbury argues that the academic ML research community should contribute more to frontier lab progress.
His argument is that algorithmic progress is compute-bottlenecked: academics just don’t have enough compute.
But automated AI researchers will have extraordinaryadvantages over human researchers, he says.
The academic ML community wasn’t even working on large language models.
Academics are way worse than automated AI researchers.
GDM is rumored to have way more experiment-based compute than OpenAI.
But it doesn't seem like GDMis massively outpacing OpenAI in terms of algorithmicprogress.
When I imagine AI systems automating AI research, I see them as compute-bottlenecked but making up for it in large part by thinking e.g.
1000 x more (and faster) than humans would.
This seems pretty doable if you’re a super strong automated researcher with very superhu-centricman intuitions.
The classic economist objection to AI automation speedingup economic growth is that different tasks are comple-                mentary.
I think the economists’ model here is correct.
But a key point is that I’m only talking about one currently-small part of the economy, rather than the economy as a whole.
I expect the level of AI capabilities to be uneven and peaky across domains.
It might be a better coder than the best engineers while still having blind spots in some subset of tasks or skills.
By the time it’s human-level at whatever its worst at, it'll already be sub-stantially superhuman at easier domains to train.
2027 /28: Proto-automated-researchers, can automate>90%.
Some remaining human bottlenecks, and hiccups, need to be worked out, but this already speeds upprogress by 3x+.
This quickly does the remaining neces-                sary “unhobbling” takes us the remainder of the way to 100% automation.
2028 /29:10x+ pace of progress →superintelligence.
Human range of intelligence is very wide, for example, with only tiny tweaks to architecture.
We’re gettinghuge OOM algorithmic gains from even just small tweaks.
As you pick the low-hanging fruit, ideas get harder to find.
A purely-algorithmic intelligence explosion would not be sustained.
The key question is essentially: for every 10x of progress,does further progress become more orlessthan 10x harder?Napkin math (along the lines of how this is done in the economic literature) helps us bound this.
A purely-algorithmic intelligence explosion would not                be sustained / would quickly fizzle out as algorith-                mic progress gets harder to find.
In response to objection 2, we can note two things: First, the mathematical condition noted above.
Second, the returns curve doesn’t even need to shake out in favor of a fully sustained chain reaction for us to get a bounded-but-many-OOM surge of algorithmicprogress.
The power of superintelligence is a topic of much discussion in the AI community.
While action isn’t fully self-sustaining, it could lead to a very                sizeable (many OOMs) one-time gain.
The AI systems we’ll likely have by the end of this decade will be unimaginably powerful.
They will be able to “think” orders of magnitude faster than humans.
Superintelligence will be like this across many domains.
It’ll find exploits in human code that are too subtle for any human to notice.
We’'ll be like high-schoolers stuck on Newtonian physicists while it’s off exploring quantum mechanics.
As we get superintelligence and apply our billions of agents to R&D, I expect explosiveprogress to broaden.
In the intelligence explosion, explosive progress was initially only in the narrow domain of automated AI research.
As we get superintelligence, and apply our billions of agents to R&D across many fields, I expect explosiveprogress to broaden.
An AI capabilities explosion.
Factories would go from human-run to AI-directed using human physical labor.
A billion superintelligences would be able to do as much R&D as possible in simulation, like AlphaFold ormanufacturing “digital twins” in the space of years.
We could see economic growth rates of around 30%/year and beyond, quite possibly multiple doublings.
With robots and AI systems being able to fully automate labor, that removes that constraint.
We could see economic growth rates of around 30%/year and beyond.
This follows fairly straightforwardly from economists’ models of economic growth.
Superintelligence could kick off another shift in growth mode.
Even early cognitive superintelligence might be enough here; perhaps some superhuman hacking scheme can de-ceive adversary militaries.
Compared to pre-superintelligence arsenals, it’ll be like 21st century mili-                taries fighting a 19th century brigade of horses and bayonets.
Whoever controls superintelligence will quite possibly have enough power to overthrow the US government.
Robots.
A common objection to claims like those here is that, even if AI can do cognitive tasks, robotics is lagging behind.
I used to be sympathetic to this, but I’ve become convinced that robots will not be a barrier.
For years people claimed that robots were a hardware problem—but robot hardware is well on its way to being solved.
Human-level AI systems, AGI, would be highly consequential in their own right.
But in some sense, they would simply be a more efficient version of what we already know.
Within just a year, we would transition to much more powerful systems.
There is a real possibility that we will lose control, as we hand off trust to AI systems.
The intelligence explosion and the immediate post-superintelligence period will be one of the most volatile, tense, dangerous, and wildestperiods ever in human history.
By the end of the decade, we’ll likely be in the midst of it.
The challenges will be immense.
It will take everything we've got to make it through.
The race to AGI won’t just play out in code and behind laptops.
It’ll be a race to mobilize America’s industrial might.
By the end of the decade, we are headed to $ 1T+ individual AI training clusters.
We’re on the path to individual training clusters costing $100s of billions.
AI is on the path to individual training clusters costing $100s of billions by 2028.
Trillions of dollars of capex will churn out 100s of millions of GPUs per year overall.
By the end of the decade, we are headed to $ 1T+ individualAI training clusters, requiring power equivalent to > 20% of US electricity production.
A 1GW, 1.4M H100-equivalent cluster (~ 2026 -cluster) is being built in Kuwait.
Microsoft and OpenAI are rumored to be working on a $ 100B cluster, slated for 2028.
The 100GW of power it’ll require is equiv-alent to > 20% of US electricity production.
The trillion-dollar cluster—+ 4OOMs from the GPT- 4cluster, the ~ 2030 training cluster on the current trend—will be a truly                extraordinary effort.
The 100GW of power it’ll require is equiv-                alent to > 20% of US electricity production.
Big tech capex is growing rapidly since ChatGPT unleashed the AI boom.
AMD fore-casts a $ 400B AI accelerator market by 2027, implying $ 700B+ of total AI spending.
My best guess is overall compute in-vestments will grow more slowly than the 3x/year largest training clusters.
AMD fore-casted a $ 400B AI accelerator market by 2027, implying $ 700B+ of total AI spending.
Of course, not all of this will be in the US, but to give areference class.
Production will probably be going to the largest training cluster in the future.
Reports suggest OpenAI was at a $ 1B revenue run rate in 2023.
That’s roughly a doubling every 6months.
If that trend holds, we should see a ~$ 10B annual run rate by late 2024 / early 2025.
One estimate puts Microsoft at ~$ 5B of in-cremental AI revenue already.
When will a big tech company (Google, Microsoft, Meta, etc.) hit a $ 100B revenue run rate from AI (products and API)?
For an average worker, that’s only a few hours a month of productivity gained.
As we “unhobble” models and they start looking more like agents, deploying them becomes much easier.
It’s hard to understate the ensuing reverberations.
Big tech at this point would be willing to go all out.
At $ 1T/year, AI investment would be about 3% of GDP.
A drop-in remote worker that automates even a fraction of white-collar/cognitive jobs would pay for the trillion-dollar cluster.
$1T/year of total annual AI investment by 2027 seems outra-                geous.
But it’s worth taking a look at other historical reference classes.
In their peak years of funding, the Manhattan and Apolloprograms reached 0.4% of GDP.
Power has become the binding constraint: there simply isn’t much spare capacity, and power contracts are usually long-term locked-in.
But it’s totally possible to do this in the United States: we have abundant natural gas.
Powering a 10GW cluster would take only a few percent of US natural gas production.
Even the 100GW cluster is surprisingly doable.
The Marcellus/Utica shale alone is producing around 36billion cubic feet a day of gas.
That would be enough to generate just under 150GW continuously with generators.
It would take about 1200 new wells for the 100GW clus-                ter.
The current rig count in the Mar cellus could build up the production base for 100GW in less than a year.
The barriers to even trillions of dollars of datacenter buildout in the US are entirely self-made.
Well-intentioned but rigid climate commitments stand in the way of the obvious, fast solution.
We’re going to drive the AGI datacenters to the Middle East, under the thumb of brutal, capricious autocrats.
Global production of AI chips is still a pretty small percent of TSMC-leading-edge production, likely less than 10%.
There’s a lot of room to grow via AI becoming alarger share ofTSMC production.
2024 production ofAI chips (~ 5-10M H100-equivalents) would already be almost enough for the $ 100s of billion cluster.
AI chip demand by 2030 will be a multiple of TSMC’s current total leading-edge logic chip capacity, just for AI.
Massive new fab investments would be neces-                sary.
TSMC has already doubled capacity in the past 5years; they’d likely need to66.
AI chip production in the U.S.
will grow at a glacial 50% CAGR.
US efforts to on-shore more AI chip production to the US would be nice, but it’s less critical than hav-watching the actual datacenter (on which the AGI lives) in the US.
The clusters that are being planned today may well be the clus-                ters AGI and superintelligence are trained and run on.
Some are rumored to be betting on building them elsewhere, especially in the Middle East.
The national interest demands that these are built in America.
Anything else creates an irreversible security risk.
In the “old days,” when AGI was still a dirty word, I used to make theoretical economic mod-els of what the path to AGI might look like.
2023 was“AI purposefullywakeup”68 behind the scenes, the most staggering techno-68.
Mainstream sell-side analysts seem to assume only 10-20% year-over-year growth in Nvidia revenue from CY 24to CY25.
The nation’s leading AI labs treat security as an afterthought.
Securing the AGI secrets andweights against the state-actor threat will be an immense effort.
At this rate, we’re basically just handing superintelligence to the CCP.
In a few years, it will be clear that AGI se-crets are the United States’ most important national defense secrets.
America’s leading AI labs self-proclaim to be building AGI.
But they do not treat it as such.
They measure their security efforts against “random tech startups,” not “key national defense projects.” As the AGI race intensifies, we will have to face the full force of foreign espionage.
The United States has an advantage in the AGI race.
But we will give up this lead if we don’t get serious about security.
Getting on this, now, is maybe even the single most im-portant thing we need to do today to ensure AGI goes well.
The capabilities of states and their intelligence agencies are extremely formidable.
Even in normal, non-all-out-AGI-race times, nation-states have been able to zero-click hack any desired iPhone and Mac with just a phone number.
Underrate state actors at your peril.
China engages in widespread industrial espionage.
The FBI director stated the PRC has a hacking operation greater than “every major nation combined” Just a couple months ago, the Attorney General announced the arrest of a Chinese national who had stolen key AI code from Google.
All it took to steal the code, without detection, was to go to Google.